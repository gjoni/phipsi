{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import numpy as np\n",
    "import utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 20 standard amino acids\n",
    "aa2idx = {'A':0, 'R':1, 'N':2, 'D':3, 'C':4, 'Q':5, 'E':6, 'G':7, 'H':8, 'I':9,\n",
    "          'L':10, 'K':11, 'M':12, 'F':13, 'P':14, 'S':15, 'T':16, 'W':17, 'Y':18, 'V':19}\n",
    "\n",
    "# load\n",
    "dataset = utils.load_phipsi()\n",
    "\n",
    "# 90% train, 10% test\n",
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=5,\n",
       "    n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCLUST = 20\n",
    "\n",
    "KM = KMeans(n_clusters=NCLUST, max_iter=5, random_state=42)\n",
    "KM.fit(np.vstack([item['avec'] for item in train]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tensorflow on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with GPU: True\n",
      "GPU available: True\n",
      "GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Built with GPU:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available:\", tf.test.is_gpu_available())\n",
    "print(\"GPU device:\", tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences & dihedral clusters\n",
    "# to one-hot representation\n",
    "for item in train:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n",
    "\n",
    "for item in test:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_ref = np.hstack([item['phi'] for item in test])\n",
    "psi_ref = np.hstack([item['psi'] for item in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from random import shuffle\n",
    "\n",
    "lr           = 0.001  # learning rate\n",
    "l2_coef      = 0.001  # L2 penalty weight\n",
    "nb_epochs    = 10\n",
    "n_layers     = 10\n",
    "n_filters    = 60\n",
    "kernel_size  = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot. params: 115880\n",
      "tot. conv1d: 12\n",
      "epoch     0 | train_loss  2.38856 | val_loss  2.22178 | rmse(phi)  44.63289 | rmse(psi)  82.28884\n",
      "epoch     1 | train_loss  2.16125 | val_loss  2.10340 | rmse(phi)  41.85695 | rmse(psi)  81.08831\n",
      "epoch     2 | train_loss  2.10102 | val_loss  2.07969 | rmse(phi)  41.18176 | rmse(psi)  80.38404\n",
      "epoch     3 | train_loss  2.08946 | val_loss  2.07757 | rmse(phi)  41.59289 | rmse(psi)  79.23122\n",
      "epoch     4 | train_loss  2.08395 | val_loss  2.08160 | rmse(phi)  41.88818 | rmse(psi)  81.00105\n",
      "epoch     5 | train_loss  2.07885 | val_loss  2.06695 | rmse(phi)  41.84675 | rmse(psi)  79.32459\n",
      "epoch     6 | train_loss  2.07822 | val_loss  2.06803 | rmse(phi)  42.01974 | rmse(psi)  80.49949\n",
      "epoch     7 | train_loss  2.07566 | val_loss  2.06913 | rmse(phi)  42.31166 | rmse(psi)  81.14040\n",
      "epoch     8 | train_loss  2.07328 | val_loss  2.06654 | rmse(phi)  41.03646 | rmse(psi)  79.68261\n",
      "epoch     9 | train_loss  2.07043 | val_loss  2.06704 | rmse(phi)  43.30840 | rmse(psi)  80.28104\n",
      "CPU times: user 13min 57s, sys: 53.3 s, total: 14min 51s\n",
      "Wall time: 12min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "relu = tf.nn.relu\n",
    "conv1d = tf.layers.conv1d\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        features = tf.placeholder(dtype=tf.int8, shape=(1, None, 20))\n",
    "        labels = tf.placeholder(dtype=tf.int8, shape=(1, None, NCLUST))\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # first convolution-activation pair\n",
    "    layers.append(relu(conv1d(tf.to_float(features), n_filters, kernel_size, padding='SAME')))\n",
    "\n",
    "    # stack of convolutional layers\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(relu(conv1d(layers[-1], n_filters, kernel_size, padding='SAME')))\n",
    "    \n",
    "    # last layer - reshape to the number of clusters\n",
    "    layers.append(relu(conv1d(layers[-1], NCLUST, kernel_size, padding='SAME')))\n",
    "\n",
    "    # loss\n",
    "    out = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.to_float(labels), logits=layers[-1])\n",
    "    loss = tf.reduce_mean(out)\n",
    "\n",
    "    # predicted probabilities for different\n",
    "    # dihedral clusters\n",
    "    prob = tf.nn.softmax(layers[-1])\n",
    "    \n",
    "    \n",
    "    vars = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                       in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "    # optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # training op\n",
    "    train_op = opt.minimize(loss+lossL2)\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    total_parameters=np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print(\"tot. params: \" + str(total_parameters))\n",
    "    print(\"tot. conv1d: \" + str(len(layers)))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for epoch in range(nb_epochs):\n",
    "            \n",
    "            train_loss = 0\n",
    "            step = 0\n",
    "            rmse_phi = 0\n",
    "            rmse_psi = 0\n",
    "            shuffle(train)\n",
    "            for item in train:\n",
    "                _, loss_value = sess.run([train_op, loss],\n",
    "                                         feed_dict={\n",
    "                                             features: item['X'],\n",
    "                                             labels: item['Y'] })\n",
    "                step += 1\n",
    "                train_loss += loss_value\n",
    "            train_loss /= step\n",
    "\n",
    "            val_loss = 0\n",
    "            step = 0\n",
    "            for item in test:\n",
    "                loss_value,pred  = sess.run([loss, prob],\n",
    "                                      feed_dict={\n",
    "                                          features: item['X'],\n",
    "                                          labels: item['Y'] })\n",
    "                step += 1\n",
    "                val_loss += loss_value\n",
    "\n",
    "                # RMSE\n",
    "                avec = np.matmul(pred.reshape((len(item['sequence']),NCLUST)), KM.cluster_centers_)\n",
    "                norm_phi = np.sqrt(np.square(avec[:,0])+np.square(avec[:,1]))\n",
    "                norm_psi = np.sqrt(np.square(avec[:,2])+np.square(avec[:,3]))\n",
    "                phi_pred = np.arctan2(avec[:,0] / norm_phi, avec[:,1] / norm_phi)\n",
    "                psi_pred = np.arctan2(avec[:,2] / norm_psi, avec[:,3] / norm_psi)\n",
    "                \n",
    "                rmse_phi += utils.ang_rmse(item['phi'], phi_pred)\n",
    "                rmse_psi += utils.ang_rmse(item['psi'], psi_pred)\n",
    "\n",
    "            val_loss /= step\n",
    "            rmse_phi /= step\n",
    "            rmse_psi /= step\n",
    "        \n",
    "            print(\"epoch {:5d} | train_loss {:8.5f} | val_loss {:8.5f} | rmse(phi) {:9.5f} | rmse(psi) {:9.5f}\".\n",
    "                  format(epoch, train_loss, val_loss, rmse_phi*180/np.pi, rmse_psi*180/np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

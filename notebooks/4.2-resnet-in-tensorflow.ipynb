{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import numpy as np\n",
    "import utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 20 standard amino acids\n",
    "aa2idx = {'A':0, 'R':1, 'N':2, 'D':3, 'C':4, 'Q':5, 'E':6, 'G':7, 'H':8, 'I':9,\n",
    "          'L':10, 'K':11, 'M':12, 'F':13, 'P':14, 'S':15, 'T':16, 'W':17, 'Y':18, 'V':19}\n",
    "\n",
    "# load\n",
    "dataset = utils.load_phipsi()\n",
    "\n",
    "# 90% train, 10% test\n",
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=5,\n",
       "    n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCLUST = 20\n",
    "\n",
    "KM = KMeans(n_clusters=NCLUST, max_iter=5, random_state=42)\n",
    "KM.fit(np.vstack([item['avec'] for item in train]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tensorflow on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with GPU: True\n",
      "GPU available: True\n",
      "GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Built with GPU:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available:\", tf.test.is_gpu_available())\n",
    "print(\"GPU device:\", tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences & dihedral clusters\n",
    "# to one-hot representation\n",
    "for item in train:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n",
    "\n",
    "for item in test:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_ref = np.hstack([item['phi'] for item in test])\n",
    "psi_ref = np.hstack([item['psi'] for item in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from random import shuffle\n",
    "\n",
    "lr           = 0.0001   # learning rate\n",
    "l2_coef      = 0.001  # L2 penalty weight\n",
    "nb_epochs    = 200\n",
    "n_layers     = 15\n",
    "n_filters    = 60\n",
    "kernel_size  = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot. params: 553880\n",
      "tot. conv1d: 32\n",
      "epoch     0 | train_loss  2.37296 | val_loss  2.29541 | rmse(phi)  27.34407 | rmse(psi)  56.65149\n",
      "epoch     1 | train_loss  2.27942 | val_loss  2.28667 | rmse(phi)  25.71244 | rmse(psi)  50.42965\n",
      "epoch     2 | train_loss  2.25689 | val_loss  2.24935 | rmse(phi)  26.58238 | rmse(psi)  52.19232\n",
      "epoch     3 | train_loss  2.24584 | val_loss  2.24225 | rmse(phi)  26.26930 | rmse(psi)  51.24101\n",
      "epoch     4 | train_loss  2.11240 | val_loss  2.05197 | rmse(phi)  25.39474 | rmse(psi)  51.05747\n",
      "epoch     5 | train_loss  2.01673 | val_loss  2.00963 | rmse(phi)  24.82359 | rmse(psi)  48.45734\n",
      "epoch     6 | train_loss  1.99854 | val_loss  2.00106 | rmse(phi)  24.59328 | rmse(psi)  48.22736\n",
      "epoch     7 | train_loss  1.96958 | val_loss  1.96413 | rmse(phi)  24.39349 | rmse(psi)  47.84904\n",
      "epoch     8 | train_loss  1.95132 | val_loss  1.96215 | rmse(phi)  24.90268 | rmse(psi)  48.73439\n",
      "epoch     9 | train_loss  1.94496 | val_loss  1.96522 | rmse(phi)  25.01083 | rmse(psi)  48.73952\n",
      "epoch    10 | train_loss  1.93491 | val_loss  1.94772 | rmse(phi)  24.75778 | rmse(psi)  47.60512\n",
      "epoch    11 | train_loss  1.93120 | val_loss  1.94858 | rmse(phi)  24.81644 | rmse(psi)  47.50551\n",
      "epoch    12 | train_loss  1.92856 | val_loss  1.94219 | rmse(phi)  24.33818 | rmse(psi)  47.19806\n",
      "epoch    13 | train_loss  1.92550 | val_loss  1.94355 | rmse(phi)  24.48468 | rmse(psi)  47.49226\n",
      "epoch    14 | train_loss  1.92291 | val_loss  1.94653 | rmse(phi)  24.15418 | rmse(psi)  47.05177\n",
      "epoch    15 | train_loss  1.92015 | val_loss  1.94149 | rmse(phi)  24.36904 | rmse(psi)  47.19807\n",
      "epoch    16 | train_loss  1.91880 | val_loss  1.94068 | rmse(phi)  24.25587 | rmse(psi)  46.65035\n",
      "epoch    17 | train_loss  1.91681 | val_loss  1.96770 | rmse(phi)  24.13390 | rmse(psi)  48.05815\n",
      "epoch    18 | train_loss  1.91489 | val_loss  1.93689 | rmse(phi)  24.43213 | rmse(psi)  47.01870\n",
      "epoch    19 | train_loss  1.91281 | val_loss  1.93440 | rmse(phi)  24.21934 | rmse(psi)  46.48748\n",
      "epoch    20 | train_loss  1.91216 | val_loss  1.93901 | rmse(phi)  24.20623 | rmse(psi)  46.67166\n",
      "epoch    21 | train_loss  1.91009 | val_loss  1.93315 | rmse(phi)  24.36719 | rmse(psi)  47.01279\n",
      "epoch    22 | train_loss  1.90914 | val_loss  1.94207 | rmse(phi)  24.12386 | rmse(psi)  46.47264\n",
      "epoch    23 | train_loss  1.90790 | val_loss  1.95289 | rmse(phi)  24.06823 | rmse(psi)  47.21626\n",
      "epoch    24 | train_loss  1.90686 | val_loss  1.93036 | rmse(phi)  24.47881 | rmse(psi)  46.65434\n",
      "epoch    25 | train_loss  1.90493 | val_loss  1.93484 | rmse(phi)  24.35107 | rmse(psi)  46.68260\n",
      "epoch    26 | train_loss  1.90334 | val_loss  1.93920 | rmse(phi)  24.13986 | rmse(psi)  46.86011\n",
      "epoch    27 | train_loss  1.90227 | val_loss  1.93552 | rmse(phi)  24.11455 | rmse(psi)  46.80717\n",
      "epoch    28 | train_loss  1.90154 | val_loss  1.92875 | rmse(phi)  24.42087 | rmse(psi)  46.68114\n",
      "epoch    29 | train_loss  1.90027 | val_loss  1.93044 | rmse(phi)  24.38796 | rmse(psi)  46.79279\n",
      "epoch    30 | train_loss  1.89919 | val_loss  1.92841 | rmse(phi)  24.11669 | rmse(psi)  46.14642\n",
      "epoch    31 | train_loss  1.89877 | val_loss  1.93470 | rmse(phi)  23.98480 | rmse(psi)  46.62996\n",
      "epoch    32 | train_loss  1.89853 | val_loss  1.92912 | rmse(phi)  24.53363 | rmse(psi)  47.03002\n",
      "epoch    33 | train_loss  1.89755 | val_loss  1.93101 | rmse(phi)  24.14227 | rmse(psi)  46.74516\n",
      "epoch    34 | train_loss  1.89594 | val_loss  1.93005 | rmse(phi)  24.19491 | rmse(psi)  46.37207\n",
      "epoch    35 | train_loss  1.89502 | val_loss  1.92787 | rmse(phi)  24.13257 | rmse(psi)  46.30697\n",
      "epoch    36 | train_loss  1.89427 | val_loss  1.92671 | rmse(phi)  24.17827 | rmse(psi)  46.53624\n",
      "epoch    37 | train_loss  1.89358 | val_loss  1.92879 | rmse(phi)  24.34306 | rmse(psi)  46.85796\n",
      "epoch    38 | train_loss  1.89244 | val_loss  1.92521 | rmse(phi)  24.14253 | rmse(psi)  46.56879\n",
      "epoch    39 | train_loss  1.89215 | val_loss  1.93587 | rmse(phi)  24.73860 | rmse(psi)  47.68463\n",
      "epoch    40 | train_loss  1.89097 | val_loss  1.92386 | rmse(phi)  24.33458 | rmse(psi)  46.51712\n",
      "epoch    41 | train_loss  1.88989 | val_loss  1.92577 | rmse(phi)  24.11422 | rmse(psi)  46.34508\n",
      "epoch    42 | train_loss  1.88995 | val_loss  1.93313 | rmse(phi)  24.06700 | rmse(psi)  46.49638\n",
      "epoch    43 | train_loss  1.88875 | val_loss  1.93091 | rmse(phi)  24.23383 | rmse(psi)  46.67000\n",
      "epoch    44 | train_loss  1.88825 | val_loss  1.93682 | rmse(phi)  24.84138 | rmse(psi)  47.95933\n",
      "epoch    45 | train_loss  1.88751 | val_loss  1.94830 | rmse(phi)  25.14971 | rmse(psi)  48.33716\n",
      "epoch    46 | train_loss  1.88690 | val_loss  1.92409 | rmse(phi)  24.22867 | rmse(psi)  46.46914\n",
      "epoch    47 | train_loss  1.88586 | val_loss  1.92588 | rmse(phi)  24.08103 | rmse(psi)  46.25854\n",
      "epoch    48 | train_loss  1.88564 | val_loss  1.92388 | rmse(phi)  24.22736 | rmse(psi)  46.58818\n",
      "epoch    49 | train_loss  1.88467 | val_loss  1.92393 | rmse(phi)  24.33981 | rmse(psi)  46.36909\n",
      "epoch    50 | train_loss  1.88405 | val_loss  1.93772 | rmse(phi)  24.22215 | rmse(psi)  46.68773\n",
      "epoch    51 | train_loss  1.88390 | val_loss  1.92769 | rmse(phi)  24.07899 | rmse(psi)  45.97387\n",
      "epoch    52 | train_loss  1.88295 | val_loss  1.92440 | rmse(phi)  24.27313 | rmse(psi)  46.34087\n",
      "epoch    53 | train_loss  1.88172 | val_loss  1.93221 | rmse(phi)  23.98084 | rmse(psi)  46.21549\n",
      "epoch    54 | train_loss  1.88192 | val_loss  1.93392 | rmse(phi)  24.52778 | rmse(psi)  47.59001\n",
      "epoch    55 | train_loss  1.88141 | val_loss  1.92947 | rmse(phi)  24.61590 | rmse(psi)  46.82597\n",
      "epoch    56 | train_loss  1.88017 | val_loss  1.92935 | rmse(phi)  24.39047 | rmse(psi)  46.84460\n",
      "epoch    57 | train_loss  1.88003 | val_loss  1.93214 | rmse(phi)  24.83545 | rmse(psi)  47.54135\n",
      "epoch    58 | train_loss  1.88001 | val_loss  1.92743 | rmse(phi)  23.95582 | rmse(psi)  45.97233\n",
      "epoch    59 | train_loss  1.87917 | val_loss  1.94967 | rmse(phi)  23.90266 | rmse(psi)  46.86466\n",
      "epoch    60 | train_loss  1.87895 | val_loss  1.93395 | rmse(phi)  24.65146 | rmse(psi)  47.56218\n",
      "epoch    61 | train_loss  1.87834 | val_loss  1.92569 | rmse(phi)  24.21025 | rmse(psi)  46.60229\n",
      "epoch    62 | train_loss  1.87666 | val_loss  1.97195 | rmse(phi)  25.50848 | rmse(psi)  50.25199\n",
      "epoch    63 | train_loss  1.87755 | val_loss  1.93573 | rmse(phi)  23.95739 | rmse(psi)  45.95848\n",
      "epoch    64 | train_loss  1.87593 | val_loss  1.92600 | rmse(phi)  24.12310 | rmse(psi)  46.20967\n",
      "epoch    65 | train_loss  1.87645 | val_loss  1.93433 | rmse(phi)  23.97264 | rmse(psi)  46.31779\n",
      "epoch    66 | train_loss  1.87565 | val_loss  1.92541 | rmse(phi)  24.33493 | rmse(psi)  46.50437\n",
      "epoch    67 | train_loss  1.87530 | val_loss  1.93530 | rmse(phi)  23.98144 | rmse(psi)  45.99642\n",
      "epoch    68 | train_loss  1.87488 | val_loss  1.94044 | rmse(phi)  23.89919 | rmse(psi)  46.25481\n",
      "epoch    69 | train_loss  1.87423 | val_loss  1.92644 | rmse(phi)  23.93324 | rmse(psi)  46.12104\n",
      "epoch    70 | train_loss  1.87335 | val_loss  1.92851 | rmse(phi)  24.33537 | rmse(psi)  46.98515\n",
      "epoch    71 | train_loss  1.87298 | val_loss  1.93343 | rmse(phi)  24.36647 | rmse(psi)  46.89661\n",
      "epoch    72 | train_loss  1.87266 | val_loss  1.92679 | rmse(phi)  24.05124 | rmse(psi)  46.02203\n",
      "epoch    73 | train_loss  1.87176 | val_loss  1.93166 | rmse(phi)  23.92574 | rmse(psi)  46.24902\n",
      "epoch    74 | train_loss  1.87212 | val_loss  1.92869 | rmse(phi)  24.01044 | rmse(psi)  46.04707\n",
      "epoch    75 | train_loss  1.87167 | val_loss  1.92572 | rmse(phi)  24.01265 | rmse(psi)  46.25502\n",
      "epoch    76 | train_loss  1.87012 | val_loss  1.93025 | rmse(phi)  24.67445 | rmse(psi)  47.16046\n",
      "epoch    77 | train_loss  1.87019 | val_loss  1.93105 | rmse(phi)  24.48506 | rmse(psi)  47.26085\n",
      "epoch    78 | train_loss  1.86924 | val_loss  1.93371 | rmse(phi)  24.38948 | rmse(psi)  46.94838\n",
      "epoch    79 | train_loss  1.86952 | val_loss  1.93382 | rmse(phi)  24.52956 | rmse(psi)  47.39865\n",
      "epoch    80 | train_loss  1.86906 | val_loss  1.92467 | rmse(phi)  24.18784 | rmse(psi)  46.35798\n",
      "epoch    81 | train_loss  1.86912 | val_loss  1.94503 | rmse(phi)  23.93460 | rmse(psi)  46.68082\n",
      "epoch    82 | train_loss  1.86832 | val_loss  1.93563 | rmse(phi)  24.19488 | rmse(psi)  47.15053\n",
      "epoch    83 | train_loss  1.86776 | val_loss  1.93138 | rmse(phi)  24.43493 | rmse(psi)  47.25633\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch    84 | train_loss  1.86721 | val_loss  1.93511 | rmse(phi)  24.36704 | rmse(psi)  47.21692\n",
      "epoch    85 | train_loss  1.86673 | val_loss  1.92828 | rmse(phi)  24.25242 | rmse(psi)  46.99362\n",
      "epoch    86 | train_loss  1.86651 | val_loss  1.93463 | rmse(phi)  24.12133 | rmse(psi)  46.70302\n",
      "epoch    87 | train_loss  1.86596 | val_loss  1.93812 | rmse(phi)  24.49997 | rmse(psi)  47.27162\n",
      "epoch    88 | train_loss  1.86562 | val_loss  1.92962 | rmse(phi)  24.14296 | rmse(psi)  46.28833\n",
      "epoch    89 | train_loss  1.86504 | val_loss  1.92854 | rmse(phi)  24.58196 | rmse(psi)  46.95330\n",
      "epoch    90 | train_loss  1.86516 | val_loss  1.93348 | rmse(phi)  23.99821 | rmse(psi)  46.03128\n",
      "epoch    91 | train_loss  1.86532 | val_loss  1.93103 | rmse(phi)  24.16485 | rmse(psi)  46.53446\n",
      "epoch    92 | train_loss  1.86414 | val_loss  1.92780 | rmse(phi)  23.99457 | rmse(psi)  46.34929\n",
      "epoch    93 | train_loss  1.86357 | val_loss  1.95206 | rmse(phi)  24.84466 | rmse(psi)  48.87040\n",
      "epoch    94 | train_loss  1.86298 | val_loss  1.93481 | rmse(phi)  23.96187 | rmse(psi)  46.25597\n",
      "epoch    95 | train_loss  1.86255 | val_loss  1.92906 | rmse(phi)  24.40898 | rmse(psi)  46.98602\n",
      "epoch    96 | train_loss  1.86210 | val_loss  1.92815 | rmse(phi)  24.32977 | rmse(psi)  46.39562\n",
      "epoch    97 | train_loss  1.86283 | val_loss  1.93006 | rmse(phi)  24.03749 | rmse(psi)  46.28712\n",
      "epoch    98 | train_loss  1.86216 | val_loss  1.93008 | rmse(phi)  23.91125 | rmse(psi)  45.73407\n",
      "epoch    99 | train_loss  1.86128 | val_loss  1.92461 | rmse(phi)  24.36251 | rmse(psi)  46.48867\n",
      "epoch   100 | train_loss  1.86102 | val_loss  1.94091 | rmse(phi)  24.06507 | rmse(psi)  46.53636\n",
      "epoch   101 | train_loss  1.86092 | val_loss  1.93492 | rmse(phi)  24.49675 | rmse(psi)  47.20669\n",
      "epoch   102 | train_loss  1.86030 | val_loss  1.92825 | rmse(phi)  24.17109 | rmse(psi)  46.63246\n",
      "epoch   103 | train_loss  1.86067 | val_loss  1.92755 | rmse(phi)  24.01426 | rmse(psi)  46.05792\n",
      "epoch   104 | train_loss  1.85987 | val_loss  1.93796 | rmse(phi)  24.30828 | rmse(psi)  46.37821\n",
      "epoch   105 | train_loss  1.85960 | val_loss  1.95846 | rmse(phi)  24.02921 | rmse(psi)  47.16840\n",
      "epoch   106 | train_loss  1.85906 | val_loss  1.93749 | rmse(phi)  24.00330 | rmse(psi)  46.62443\n",
      "epoch   107 | train_loss  1.85885 | val_loss  1.94003 | rmse(phi)  24.54309 | rmse(psi)  47.47766\n",
      "epoch   108 | train_loss  1.85889 | val_loss  1.93004 | rmse(phi)  24.18069 | rmse(psi)  46.55200\n",
      "epoch   109 | train_loss  1.85749 | val_loss  1.93639 | rmse(phi)  24.50335 | rmse(psi)  46.99450\n",
      "epoch   110 | train_loss  1.85832 | val_loss  1.94030 | rmse(phi)  23.96742 | rmse(psi)  46.39495\n",
      "epoch   111 | train_loss  1.85816 | val_loss  1.94854 | rmse(phi)  24.09600 | rmse(psi)  46.77955\n",
      "epoch   112 | train_loss  1.85714 | val_loss  1.93512 | rmse(phi)  24.40099 | rmse(psi)  46.97960\n",
      "epoch   113 | train_loss  1.85710 | val_loss  1.93185 | rmse(phi)  24.10241 | rmse(psi)  46.56512\n",
      "epoch   114 | train_loss  1.85684 | val_loss  1.92672 | rmse(phi)  24.15220 | rmse(psi)  46.38504\n",
      "epoch   115 | train_loss  1.85667 | val_loss  1.93033 | rmse(phi)  24.42004 | rmse(psi)  46.69805\n",
      "epoch   116 | train_loss  1.85634 | val_loss  1.94015 | rmse(phi)  24.22918 | rmse(psi)  46.97246\n",
      "epoch   117 | train_loss  1.85582 | val_loss  1.93567 | rmse(phi)  24.52115 | rmse(psi)  47.28297\n",
      "epoch   118 | train_loss  1.85594 | val_loss  1.93103 | rmse(phi)  24.04178 | rmse(psi)  46.41600\n",
      "epoch   119 | train_loss  1.85542 | val_loss  1.93969 | rmse(phi)  23.99942 | rmse(psi)  46.25617\n",
      "epoch   120 | train_loss  1.85508 | val_loss  1.94805 | rmse(phi)  24.79567 | rmse(psi)  48.34679\n",
      "epoch   121 | train_loss  1.85557 | val_loss  1.94593 | rmse(phi)  24.46637 | rmse(psi)  47.65051\n",
      "epoch   122 | train_loss  1.85466 | val_loss  1.94989 | rmse(phi)  24.91771 | rmse(psi)  48.37525\n",
      "epoch   123 | train_loss  1.85421 | val_loss  1.93100 | rmse(phi)  24.17967 | rmse(psi)  46.48094\n",
      "epoch   124 | train_loss  1.85399 | val_loss  1.93971 | rmse(phi)  24.47458 | rmse(psi)  47.03073\n",
      "epoch   125 | train_loss  1.85371 | val_loss  1.93836 | rmse(phi)  24.15208 | rmse(psi)  46.64163\n",
      "epoch   126 | train_loss  1.85364 | val_loss  1.94593 | rmse(phi)  24.53278 | rmse(psi)  47.73090\n",
      "epoch   127 | train_loss  1.85352 | val_loss  1.93840 | rmse(phi)  24.03111 | rmse(psi)  46.21768\n",
      "epoch   128 | train_loss  1.85378 | val_loss  1.93591 | rmse(phi)  24.21574 | rmse(psi)  47.02443\n",
      "epoch   129 | train_loss  1.85254 | val_loss  1.95225 | rmse(phi)  24.69914 | rmse(psi)  48.40450\n",
      "epoch   130 | train_loss  1.85280 | val_loss  1.94663 | rmse(phi)  24.45999 | rmse(psi)  47.87401\n",
      "epoch   131 | train_loss  1.85225 | val_loss  1.96717 | rmse(phi)  23.91072 | rmse(psi)  46.83106\n",
      "epoch   132 | train_loss  1.85189 | val_loss  1.93768 | rmse(phi)  23.90519 | rmse(psi)  46.46511\n",
      "epoch   133 | train_loss  1.85204 | val_loss  1.93440 | rmse(phi)  24.28634 | rmse(psi)  47.47824\n",
      "epoch   134 | train_loss  1.85138 | val_loss  1.93182 | rmse(phi)  24.18317 | rmse(psi)  46.54674\n",
      "epoch   135 | train_loss  1.85156 | val_loss  1.93533 | rmse(phi)  24.34020 | rmse(psi)  46.55527\n",
      "epoch   136 | train_loss  1.85109 | val_loss  1.93825 | rmse(phi)  24.12202 | rmse(psi)  46.73658\n",
      "epoch   137 | train_loss  1.85090 | val_loss  1.93340 | rmse(phi)  24.25785 | rmse(psi)  46.92645\n",
      "epoch   138 | train_loss  1.85037 | val_loss  1.93514 | rmse(phi)  24.12373 | rmse(psi)  46.65144\n",
      "epoch   139 | train_loss  1.85056 | val_loss  1.93992 | rmse(phi)  24.37559 | rmse(psi)  47.23276\n",
      "epoch   140 | train_loss  1.85022 | val_loss  1.95144 | rmse(phi)  23.92368 | rmse(psi)  46.39985\n",
      "epoch   141 | train_loss  1.84947 | val_loss  1.95679 | rmse(phi)  24.71628 | rmse(psi)  48.49181\n",
      "epoch   142 | train_loss  1.84964 | val_loss  1.93701 | rmse(phi)  24.20643 | rmse(psi)  46.73455\n",
      "epoch   143 | train_loss  1.85011 | val_loss  1.93322 | rmse(phi)  24.08788 | rmse(psi)  46.60288\n",
      "epoch   144 | train_loss  1.84878 | val_loss  1.93078 | rmse(phi)  24.45387 | rmse(psi)  46.82266\n",
      "epoch   145 | train_loss  1.84903 | val_loss  1.94033 | rmse(phi)  24.55715 | rmse(psi)  47.31982\n",
      "epoch   146 | train_loss  1.84839 | val_loss  1.93730 | rmse(phi)  24.08097 | rmse(psi)  46.63071\n",
      "epoch   147 | train_loss  1.84845 | val_loss  1.95894 | rmse(phi)  23.92394 | rmse(psi)  46.94141\n",
      "epoch   148 | train_loss  1.84868 | val_loss  1.93943 | rmse(phi)  24.44987 | rmse(psi)  47.38679\n",
      "epoch   149 | train_loss  1.84806 | val_loss  1.94389 | rmse(phi)  23.94441 | rmse(psi)  46.40153\n",
      "epoch   150 | train_loss  1.84793 | val_loss  1.94483 | rmse(phi)  24.02818 | rmse(psi)  46.58194\n",
      "epoch   151 | train_loss  1.84785 | val_loss  1.94243 | rmse(phi)  24.06993 | rmse(psi)  46.62099\n",
      "epoch   152 | train_loss  1.84743 | val_loss  1.94089 | rmse(phi)  24.07148 | rmse(psi)  46.93884\n",
      "epoch   153 | train_loss  1.84717 | val_loss  1.94198 | rmse(phi)  24.12396 | rmse(psi)  46.52546\n",
      "epoch   154 | train_loss  1.84705 | val_loss  1.95451 | rmse(phi)  24.42678 | rmse(psi)  47.60220\n",
      "epoch   155 | train_loss  1.84711 | val_loss  1.94342 | rmse(phi)  24.43672 | rmse(psi)  48.08191\n",
      "epoch   156 | train_loss  1.84656 | val_loss  1.94197 | rmse(phi)  24.13976 | rmse(psi)  47.09305\n",
      "epoch   157 | train_loss  1.84548 | val_loss  1.93304 | rmse(phi)  23.95177 | rmse(psi)  46.48630\n",
      "epoch   158 | train_loss  1.84605 | val_loss  1.94457 | rmse(phi)  23.95375 | rmse(psi)  46.68751\n",
      "epoch   159 | train_loss  1.84588 | val_loss  1.94826 | rmse(phi)  23.96942 | rmse(psi)  46.80464\n",
      "epoch   160 | train_loss  1.84607 | val_loss  1.93571 | rmse(phi)  24.39457 | rmse(psi)  46.83174\n",
      "epoch   161 | train_loss  1.84611 | val_loss  1.94128 | rmse(phi)  24.33073 | rmse(psi)  47.54053\n",
      "epoch   162 | train_loss  1.84523 | val_loss  1.94104 | rmse(phi)  24.09253 | rmse(psi)  46.60843\n",
      "epoch   163 | train_loss  1.84462 | val_loss  1.93395 | rmse(phi)  24.09657 | rmse(psi)  46.66403\n",
      "epoch   164 | train_loss  1.84487 | val_loss  1.94180 | rmse(phi)  24.59495 | rmse(psi)  48.01367\n",
      "epoch   165 | train_loss  1.84470 | val_loss  1.93631 | rmse(phi)  24.12762 | rmse(psi)  46.87739\n",
      "epoch   166 | train_loss  1.84513 | val_loss  1.93334 | rmse(phi)  24.15963 | rmse(psi)  46.62745\n",
      "epoch   167 | train_loss  1.84413 | val_loss  1.95589 | rmse(phi)  24.95475 | rmse(psi)  49.23717\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch   168 | train_loss  1.84487 | val_loss  1.93298 | rmse(phi)  24.12432 | rmse(psi)  46.76008\n",
      "epoch   169 | train_loss  1.84349 | val_loss  1.94331 | rmse(phi)  24.21465 | rmse(psi)  47.03078\n",
      "epoch   170 | train_loss  1.84363 | val_loss  1.94712 | rmse(phi)  24.72603 | rmse(psi)  48.30874\n",
      "epoch   171 | train_loss  1.84377 | val_loss  1.93484 | rmse(phi)  24.12907 | rmse(psi)  46.81352\n",
      "epoch   172 | train_loss  1.84360 | val_loss  1.94445 | rmse(phi)  24.08187 | rmse(psi)  46.65103\n",
      "epoch   173 | train_loss  1.84263 | val_loss  1.96497 | rmse(phi)  24.89380 | rmse(psi)  49.63749\n",
      "epoch   174 | train_loss  1.84289 | val_loss  1.94046 | rmse(phi)  24.10796 | rmse(psi)  46.97586\n",
      "epoch   175 | train_loss  1.84327 | val_loss  1.95038 | rmse(phi)  23.88133 | rmse(psi)  46.69677\n",
      "epoch   176 | train_loss  1.84253 | val_loss  1.93757 | rmse(phi)  24.35848 | rmse(psi)  47.33553\n",
      "epoch   177 | train_loss  1.84243 | val_loss  1.93334 | rmse(phi)  24.37053 | rmse(psi)  47.05393\n",
      "epoch   178 | train_loss  1.84175 | val_loss  1.93856 | rmse(phi)  24.23204 | rmse(psi)  46.62782\n",
      "epoch   179 | train_loss  1.84236 | val_loss  1.94855 | rmse(phi)  24.12391 | rmse(psi)  46.33311\n",
      "epoch   180 | train_loss  1.84195 | val_loss  1.93700 | rmse(phi)  24.11240 | rmse(psi)  46.88464\n",
      "epoch   181 | train_loss  1.84210 | val_loss  1.94426 | rmse(phi)  24.27076 | rmse(psi)  47.01061\n",
      "epoch   182 | train_loss  1.84214 | val_loss  1.93546 | rmse(phi)  24.19309 | rmse(psi)  47.03492\n",
      "epoch   183 | train_loss  1.84071 | val_loss  1.93506 | rmse(phi)  24.29487 | rmse(psi)  46.87991\n",
      "epoch   184 | train_loss  1.84098 | val_loss  1.93869 | rmse(phi)  24.37150 | rmse(psi)  47.08759\n",
      "epoch   185 | train_loss  1.84074 | val_loss  1.95474 | rmse(phi)  23.91053 | rmse(psi)  47.02745\n",
      "epoch   186 | train_loss  1.84101 | val_loss  1.93633 | rmse(phi)  24.51289 | rmse(psi)  47.72967\n",
      "epoch   187 | train_loss  1.84039 | val_loss  1.94146 | rmse(phi)  24.39345 | rmse(psi)  47.42718\n",
      "epoch   188 | train_loss  1.84112 | val_loss  1.94108 | rmse(phi)  24.17636 | rmse(psi)  47.14632\n",
      "epoch   189 | train_loss  1.83992 | val_loss  1.94812 | rmse(phi)  23.96439 | rmse(psi)  46.75310\n",
      "epoch   190 | train_loss  1.84022 | val_loss  1.95662 | rmse(phi)  24.81788 | rmse(psi)  48.61030\n",
      "epoch   191 | train_loss  1.84087 | val_loss  1.94424 | rmse(phi)  24.10260 | rmse(psi)  46.84677\n",
      "epoch   192 | train_loss  1.84059 | val_loss  1.94733 | rmse(phi)  23.98638 | rmse(psi)  46.75092\n",
      "epoch   193 | train_loss  1.83984 | val_loss  1.94312 | rmse(phi)  24.19054 | rmse(psi)  46.99005\n",
      "epoch   194 | train_loss  1.83864 | val_loss  1.94812 | rmse(phi)  24.11562 | rmse(psi)  46.78783\n",
      "epoch   195 | train_loss  1.83966 | val_loss  1.94629 | rmse(phi)  24.24211 | rmse(psi)  46.84484\n",
      "epoch   196 | train_loss  1.83915 | val_loss  1.93735 | rmse(phi)  24.23792 | rmse(psi)  47.12355\n",
      "epoch   197 | train_loss  1.83959 | val_loss  1.94410 | rmse(phi)  24.28211 | rmse(psi)  47.14953\n",
      "epoch   198 | train_loss  1.83920 | val_loss  1.95093 | rmse(phi)  24.65360 | rmse(psi)  47.92853\n",
      "epoch   199 | train_loss  1.83877 | val_loss  1.93809 | rmse(phi)  24.23401 | rmse(psi)  47.00541\n",
      "CPU times: user 9h 22min 24s, sys: 18min 20s, total: 9h 40min 45s\n",
      "Wall time: 8h 38min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "relu = tf.nn.relu\n",
    "conv1d = tf.layers.conv1d\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        features = tf.placeholder(dtype=tf.int8, shape=(1, None, 20))\n",
    "        labels = tf.placeholder(dtype=tf.int8, shape=(1, None, NCLUST))\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # first convolution-activation pair\n",
    "    layers.append(relu(conv1d(tf.to_float(features), n_filters, kernel_size, padding='SAME')))\n",
    "\n",
    "    # stack of residual layers\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(relu(conv1d(layers[-1], n_filters, kernel_size, padding='SAME')))\n",
    "        layers.append(relu(conv1d(layers[-1], n_filters, kernel_size, padding='SAME') + layers[-2]))\n",
    "    \n",
    "    # last layer - output\n",
    "    layers.append(relu(conv1d(layers[-1], NCLUST, kernel_size, padding='SAME')))\n",
    "\n",
    "    # loss\n",
    "    out = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.to_float(labels), logits=layers[-1])\n",
    "    loss = tf.reduce_mean(out)\n",
    "\n",
    "    # predicted probabilities for different\n",
    "    # dihedral clusters\n",
    "    prob = tf.nn.softmax(layers[-1])\n",
    "    \n",
    "    \n",
    "    vars = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                       in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "    # optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # training op\n",
    "    train_op = opt.minimize(loss+lossL2)\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    total_parameters=np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print(\"tot. params: \" + str(total_parameters))\n",
    "    print(\"tot. conv1d: \" + str(len(layers)))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for epoch in range(nb_epochs):\n",
    "            \n",
    "            train_loss = 0\n",
    "            step = 0\n",
    "            rmse_phi = 0\n",
    "            rmse_psi = 0\n",
    "            shuffle(train)\n",
    "            for item in train:\n",
    "                _, loss_value = sess.run([train_op, loss],\n",
    "                                         feed_dict={\n",
    "                                             features: item['X'],\n",
    "                                             labels: item['Y'] })\n",
    "                step += 1\n",
    "                train_loss += loss_value\n",
    "            train_loss /= step\n",
    "\n",
    "            val_loss = 0\n",
    "            step = 0\n",
    "            for item in test:\n",
    "                loss_value,pred  = sess.run([loss, prob],\n",
    "                                      feed_dict={\n",
    "                                          features: item['X'],\n",
    "                                          labels: item['Y'] })\n",
    "                step += 1\n",
    "                val_loss += loss_value\n",
    "\n",
    "                # RMSE\n",
    "                avec = np.matmul(pred.reshape((len(item['sequence']),NCLUST)), KM.cluster_centers_)\n",
    "                norm_phi = np.sqrt(np.square(avec[:,0])+np.square(avec[:,1]))\n",
    "                norm_psi = np.sqrt(np.square(avec[:,2])+np.square(avec[:,3]))\n",
    "                phi_pred = np.arctan2(avec[:,0] / norm_phi, avec[:,1] / norm_phi)\n",
    "                psi_pred = np.arctan2(avec[:,2] / norm_psi, avec[:,3] / norm_psi)\n",
    "                \n",
    "                rmse_phi += utils.ang_mae(item['phi'], phi_pred)\n",
    "                rmse_psi += utils.ang_mae(item['psi'], psi_pred)\n",
    "\n",
    "            val_loss /= step\n",
    "            rmse_phi /= step\n",
    "            rmse_psi /= step\n",
    "        \n",
    "            print(\"epoch {:5d} | train_loss {:8.5f} | val_loss {:8.5f} | rmse(phi) {:9.5f} | rmse(psi) {:9.5f}\".\n",
    "                  format(epoch, train_loss, val_loss, rmse_phi*180/np.pi, rmse_psi*180/np.pi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

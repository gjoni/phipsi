{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip, json\n",
    "import numpy as np\n",
    "import utils \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 20 standard amino acids\n",
    "aa2idx = {'A':0, 'R':1, 'N':2, 'D':3, 'C':4, 'Q':5, 'E':6, 'G':7, 'H':8, 'I':9,\n",
    "          'L':10, 'K':11, 'M':12, 'F':13, 'P':14, 'S':15, 'T':16, 'W':17, 'Y':18, 'V':19}\n",
    "\n",
    "# load\n",
    "dataset = utils.load_phipsi()\n",
    "\n",
    "# 90% train, 10% test\n",
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=5,\n",
       "    n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NCLUST = 20\n",
    "\n",
    "KM = KMeans(n_clusters=NCLUST, max_iter=5, random_state=42)\n",
    "KM.fit(np.vstack([item['avec'] for item in train]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=5,\n",
       "    n_clusters=20, n_init=10, n_jobs=None, precompute_distances='auto',\n",
       "    random_state=42, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "NCLUST = 20\n",
    "with open('phipsi_km20.pkl', 'rb') as f:\n",
    "    KM = pickle.load(f)\n",
    "KM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test tensorflow on GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built with GPU: True\n",
      "GPU available: True\n",
      "GPU device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"Built with GPU:\", tf.test.is_built_with_cuda())\n",
    "print(\"GPU available:\", tf.test.is_gpu_available())\n",
    "print(\"GPU device:\", tf.test.gpu_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert sequences & dihedral clusters\n",
    "# to one-hot representation\n",
    "for item in train:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n",
    "\n",
    "for item in test:\n",
    "    item['X'] = np.eye(20)[item['sequence']]\n",
    "    item['Y'] = np.eye(NCLUST)[np.array(KM.predict(item['avec']), dtype=np.int8)]\n",
    "    item['X'] = item['X'][np.newaxis]\n",
    "    item['Y'] = item['Y'][np.newaxis]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi_ref = np.hstack([item['phi'] for item in test])\n",
    "psi_ref = np.hstack([item['psi'] for item in test])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6522007457633086\n"
     ]
    }
   ],
   "source": [
    "# entropy of the background distribution\n",
    "X_pred = KM.predict(np.vstack([item['avec'] for item in test]))\n",
    "a = np.sum(np.eye(NCLUST)[X_pred], axis=0)/X_pred.shape[0]\n",
    "s0=-np.sum(np.log(a)*a)\n",
    "print(s0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "from random import shuffle\n",
    "\n",
    "lr           = 0.0001   # learning rate\n",
    "l2_coef      = 0.001  # L2 penalty weight\n",
    "nb_epochs    = 10\n",
    "n_layers     = 5\n",
    "n_filters    = 60\n",
    "kernel_size  = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tot. params: 192680\n",
      "tot. layers: 12\n",
      "epoch     0 | train_loss  2.18220 | val_loss  2.12933 | mae(phi)  25.47346 | mae(psi)  51.59488 | s_loss   0.54863\n",
      "epoch     1 | train_loss  2.10700 | val_loss  2.10240 | mae(phi)  25.85320 | mae(psi)  50.91024 | s_loss   0.49199\n",
      "epoch     2 | train_loss  2.08838 | val_loss  2.08943 | mae(phi)  25.07425 | mae(psi)  49.85411 | s_loss   0.61191\n",
      "epoch     3 | train_loss  2.07980 | val_loss  2.08696 | mae(phi)  25.31871 | mae(psi)  49.97214 | s_loss   0.57210\n",
      "epoch     4 | train_loss  2.07235 | val_loss  2.07344 | mae(phi)  25.10412 | mae(psi)  49.35855 | s_loss   0.56064\n",
      "epoch     5 | train_loss  2.06734 | val_loss  2.07283 | mae(phi)  25.10174 | mae(psi)  49.13631 | s_loss   0.58730\n",
      "epoch     6 | train_loss  2.06392 | val_loss  2.07042 | mae(phi)  24.97360 | mae(psi)  49.14294 | s_loss   0.57989\n",
      "epoch     7 | train_loss  2.06083 | val_loss  2.07251 | mae(phi)  24.69616 | mae(psi)  48.96504 | s_loss   0.63968\n",
      "epoch     8 | train_loss  2.05772 | val_loss  2.06950 | mae(phi)  24.77084 | mae(psi)  49.10443 | s_loss   0.63407\n",
      "epoch     9 | train_loss  2.05554 | val_loss  2.06341 | mae(phi)  24.78353 | mae(psi)  49.16222 | s_loss   0.60501\n",
      "CPU times: user 14min 29s, sys: 46 s, total: 15min 15s\n",
      "Wall time: 12min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "relu = tf.nn.elu\n",
    "conv1d = tf.layers.conv1d\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        features = tf.placeholder(dtype=tf.int8, shape=(1, None, 20))\n",
    "        labels = tf.placeholder(dtype=tf.int8, shape=(1, None, NCLUST))\n",
    "\n",
    "    layers = []\n",
    "\n",
    "    # first convolution-activation pair\n",
    "    layers.append(relu(conv1d(tf.to_float(features), n_filters, kernel_size, padding='SAME')))\n",
    "\n",
    "    # stack of residual layers\n",
    "    for _ in range(n_layers):\n",
    "        layers.append(relu(conv1d(layers[-1], n_filters, kernel_size, padding='SAME')))\n",
    "        layers.append(relu(conv1d(layers[-1], n_filters, kernel_size, padding='SAME') + layers[-2]))\n",
    "    \n",
    "    # last layer - output\n",
    "    layers.append(conv1d(layers[-1], NCLUST, kernel_size, padding='SAME'))\n",
    "\n",
    "    # loss\n",
    "    out = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.to_float(labels), logits=layers[-1])\n",
    "    loss = tf.reduce_mean(out)\n",
    "\n",
    "    # predicted probabilities for different\n",
    "    # dihedral clusters\n",
    "    prob = tf.nn.softmax(layers[-1])\n",
    "    \n",
    "    \n",
    "    vars = tf.trainable_variables()\n",
    "    lossL2 = tf.add_n([tf.nn.l2_loss(v) for v in vars if v.name not\n",
    "                       in ['bias', 'gamma', 'b', 'g', 'beta']]) * l2_coef\n",
    "    # optimizer\n",
    "    opt = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "\n",
    "    # training op\n",
    "    train_op = opt.minimize(loss+lossL2)\n",
    "\n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "\n",
    "    total_parameters=np.sum([np.prod(v.get_shape().as_list()) for v in tf.trainable_variables()])\n",
    "    print(\"tot. params: \" + str(total_parameters))\n",
    "    print(\"tot. layers: \" + str(len(layers)))\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        \n",
    "        for epoch in range(nb_epochs):\n",
    "            \n",
    "            train_loss = 0\n",
    "            step = 0\n",
    "            rmse_phi = 0\n",
    "            rmse_psi = 0\n",
    "            shuffle(train)\n",
    "            for item in train:\n",
    "                _, loss_value = sess.run([train_op, loss],\n",
    "                                         feed_dict={\n",
    "                                             features: item['X'],\n",
    "                                             labels: item['Y'] })\n",
    "                step += 1\n",
    "                train_loss += loss_value\n",
    "            train_loss /= step\n",
    "\n",
    "            val_loss = 0\n",
    "            step = 0\n",
    "            s = 0\n",
    "            for item in test:\n",
    "                loss_value,pred  = sess.run([loss, prob],\n",
    "                                      feed_dict={\n",
    "                                          features: item['X'],\n",
    "                                          labels: item['Y'] })\n",
    "                step += 1\n",
    "                val_loss += loss_value\n",
    "\n",
    "                # RMSE\n",
    "                avec = np.matmul(pred.reshape((len(item['sequence']),NCLUST)), KM.cluster_centers_)\n",
    "                norm_phi = np.sqrt(np.square(avec[:,0])+np.square(avec[:,1]))\n",
    "                norm_psi = np.sqrt(np.square(avec[:,2])+np.square(avec[:,3]))\n",
    "                phi_pred = np.arctan2(avec[:,0] / norm_phi, avec[:,1] / norm_phi)\n",
    "                psi_pred = np.arctan2(avec[:,2] / norm_psi, avec[:,3] / norm_psi)\n",
    "                \n",
    "                rmse_phi += utils.ang_mae(item['phi'], phi_pred)\n",
    "                rmse_psi += utils.ang_mae(item['psi'], psi_pred)\n",
    "                \n",
    "                s += np.average(np.sum(-np.log(pred)*pred, axis=-1))\n",
    "\n",
    "            val_loss /= step\n",
    "            rmse_phi /= step\n",
    "            rmse_psi /= step\n",
    "        \n",
    "            print(\"epoch {:5d} | train_loss {:8.5f} | val_loss {:8.5f} | mae(phi) {:9.5f} | mae(psi) {:9.5f} | s_loss {:9.5f}\".\n",
    "                  format(epoch, train_loss, val_loss, rmse_phi*180/np.pi, rmse_psi*180/np.pi, s0 - s/len(test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of CPUs to be used\n",
    "NCPU=4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "aa2idx = {'A':0, 'R':1, 'N':2, 'D':3, 'C':4, 'Q':5, 'E':6, 'G':7, 'H':8, 'I':9,\n",
    "          'L':10, 'K':11, 'M':12, 'F':13, 'P':14, 'S':15, 'T':16, 'W':17, 'Y':18, 'V':19}\n",
    "\n",
    "# read .json file\n",
    "with gzip.open('../data/phipsi.json.gz', 'rb') as f:\n",
    "    dataset = json.loads(f.read().decode('utf-8'))\n",
    "\n",
    "# reduse dataset to a list for simpler access\n",
    "dataset = dataset['phipsi10882']\n",
    "\n",
    "# convert data to numpy arrays skipping first and last residues\n",
    "for item in dataset:\n",
    "    n = len(item['sequence'])\n",
    "    item['sequence'] = np.array([aa2idx[aa] for aa in item['sequence'][1:n-1]], dtype=np.int8)\n",
    "    item['phi'] = np.array(item['phi'], dtype=np.float32)[1:n-1]\n",
    "    item['psi'] = np.array(item['psi'], dtype=np.float32)[1:n-1]\n",
    "    \n",
    "    # convert (phi,psi) to their sin() and cos()\n",
    "    # (4 numbers per angle pair)\n",
    "    item['avec'] = np.vstack([\n",
    "        np.sin(item['phi']).T,\n",
    "        np.cos(item['phi']).T,\n",
    "        np.sin(item['psi']).T,\n",
    "        np.cos(item['psi']).T ]).T\n",
    "\n",
    "# 90% train, 10% test\n",
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 760 ms, sys: 138 ms, total: 898 ms\n",
      "Wall time: 17.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# split train set into 20 clusters\n",
    "KMEANS = KMeans(n_clusters=20, max_iter=5, n_jobs=NCPU)\n",
    "KMEANS.fit(np.vstack([item['avec'] for item in train]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Help functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign each (phi,psi) to a cluster\n",
    "# (KMEANS stores clustering results)\n",
    "def set_clusters(train, test, KMEANS):\n",
    "    for subset in (train, test):\n",
    "        for item in subset:\n",
    "            item['abin'] = np.array(KMEANS.predict(item['avec']), dtype=np.int8)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split all sequences into chunks of size WINDOW\n",
    "def set_window(train, test, WINDOW):\n",
    "    \n",
    "    for subset in (train, test):\n",
    "        for item in subset:\n",
    "            l = len(item['sequence'])\n",
    "\n",
    "            abin = item['abin']\n",
    "            seq = item['sequence']\n",
    "\n",
    "            # for every window, pick the element in the middle and\n",
    "            # save corresponding dihedral cluster ID in item['Y']\n",
    "            item['Y'] = np.hstack([item[WINDOW//2] \n",
    "                                   for shift in range(0,WINDOW,1) \n",
    "                                   for item in np.split(abin[shift:],range(0,l,WINDOW)) \n",
    "                                   if len(item) == WINDOW])\n",
    "\n",
    "            # use 1-hot encoding for every sequence chunk\n",
    "            seq_chunks = np.vstack([item for shift in range(0,WINDOW,1) \n",
    "                                    for item in np.split(seq[shift:],range(0,l,WINDOW)) \n",
    "                                    if len(item) == WINDOW])\n",
    "            item['X'] = np.array(np.eye(20)[seq_chunks], dtype=np.int8).reshape((seq_chunks.shape[0],-1))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy loss\n",
    "def entropy_loss(Y_train, Y_test_pred):\n",
    "\n",
    "    # background distribution\n",
    "    _,counts = np.unique(Y_train, return_counts=True)\n",
    "    counts = counts / Y_train.shape[0]\n",
    "    H0 = np.sum(counts * np.log(counts))\n",
    "    \n",
    "    # predicted distribution\n",
    "    H1 = np.average(np.sum(Y_test_pred * np.log(Y_test_pred), axis=1))\n",
    "    \n",
    "    return H1 - H0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomness in clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means clustering is stochastic: each run of the algorithm can produce a different set of clusters. Here we check how this randomness affects the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "WINDOW = 15\n",
    "\n",
    "niter=50\n",
    "\n",
    "KMEANS = []\n",
    "\n",
    "for i in range(niter):\n",
    "    \n",
    "    # cluster\n",
    "    KM = KMeans(n_clusters=20, max_iter=5, n_jobs=NCPU)\n",
    "    KM.fit(np.vstack([item['avec'] for item in train[::5]]))\n",
    "    \n",
    "    set_clusters(train, test, KM)\n",
    "    KMEANS.append(KM)\n",
    "\n",
    "    set_window(train, test, WINDOW)\n",
    "    \n",
    "    X_train = np.vstack([item['X'] for item in train[::5]])\n",
    "    Y_train = np.hstack([item['Y'] for item in train[::5]])\n",
    "    X_test = np.vstack([item['X'] for item in test])\n",
    "    Y_test = np.hstack([item['Y'] for item in test])\n",
    "\n",
    "    sgd = SGDClassifier(max_iter=10, tol=1e-3, loss='log', n_jobs=NCPU)\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    score_train = sgd.score(X_train, Y_train)\n",
    "    score_test = sgd.score(X_test, Y_test)\n",
    "    \n",
    "    Y_test_pred = sgd.predict_proba(X_test)\n",
    "    \n",
    "    loss = log_loss(Y_test, Y_test_pred)\n",
    "    hloss = entropy_loss(Y_train, Y_test_pred)\n",
    "\n",
    "    print(i, score_train, score_test, loss, hloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IDX=22\n",
    "\n",
    "cent = KMEANS[IDX].cluster_centers_\n",
    "cphi = np.arctan2(cent[:,0], cent[:,1])\n",
    "cpsi = np.arctan2(cent[:,2], cent[:,3])\n",
    "\n",
    "set_clusters(train, test, KMEANS[IDX])\n",
    "set_window(train, test, WINDOW)\n",
    "\n",
    "X_train = np.vstack([item['X'] for item in train])\n",
    "\n",
    "X_pred = KMEANS[IDX].predict(X_train)\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# stack phi and psi from all proteins together\n",
    "# (again, use 1/50 of the full set)\n",
    "phi = np.hstack([item['phi'] for item in train[::50]])\n",
    "psi = np.hstack([item['psi'] for item in train[::50]])\n",
    "\n",
    "matplotlib.rcParams['image.cmap'] = 'jet'\n",
    "\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax1 = fig.add_subplot(111)\n",
    "plt.xlim([-np.pi, np.pi])\n",
    "plt.ylim([-np.pi, np.pi])\n",
    "plt.xlabel('phi', fontsize=20)\n",
    "plt.ylabel('psi', fontsize=20)\n",
    "plt.scatter(phi, psi, c=X_pred, s=10)\n",
    "plt.scatter(cphi, cpsi, c='black', s=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.20217837991035137 0.2021506452770061 2.5107150487896885 0.18766689267163228\n",
      "3 0.22520720447781836 0.2248825579633278 2.408992948582559 0.25944257705865903\n",
      "5 0.2372721609502339 0.23753324326850364 2.367115954279289 0.284624269940168\n",
      "7 0.2460299658855522 0.2459778395252307 2.340816646838272 0.30568972733227096\n",
      "9 0.24972906546939408 0.2501666450244557 2.3276969926992725 0.31685486102714444\n",
      "11 0.2518358020723536 0.2517128974184189 2.3197511910307336 0.3333097128825049\n",
      "13 0.25344224232991475 0.25252124195981895 2.3147558753519286 0.33847461266227974\n",
      "15 0.2534481779856406 0.25261474591974914 2.3125384676300476 0.3337847198207773\n",
      "17 0.2537981952915051 0.25302044818683145 2.3127911884813774 0.33422220796936264\n",
      "19 0.2540800105637916 0.25308894339965476 2.312523284864987 0.3369036304298625\n",
      "21 0.254259250519646 0.25371395013809744 2.3135956765184704 0.33628331097636144\n",
      "23 0.2536866170799526 0.2528454380306232 2.314634574646013 0.34540779475231087\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/projects/omics/prog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    584\u001b[0m                          \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m                          \u001b[0mcoef_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcoef_init\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mintercept_init\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mintercept_init\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                          sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/omics/prog/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, alpha, C, loss, learning_rate, coef_init, intercept_init, sample_weight)\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 418\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"C\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    419\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/projects/omics/prog/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    571\u001b[0m     X = check_array(X, accept_sparse, dtype, order, copy, force_all_finite,\n\u001b[1;32m    572\u001b[0m                     \u001b[0mensure_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_nd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensure_min_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m                     ensure_min_features, warn_on_dtype, estimator)\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/projects/omics/prog/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "KMEANS = KMeans(n_clusters=20, max_iter=5, n_jobs=NCPU)\n",
    "KMEANS.fit(np.vstack([item['avec'] for item in train]))\n",
    "\n",
    "set_clusters(train, test, KMEANS)\n",
    "\n",
    "for WINDOW in range(1,33,2):\n",
    "    set_window(train, test, WINDOW)\n",
    "    \n",
    "    X_train = np.vstack([item['X'] for item in train])\n",
    "    Y_train = np.hstack([item['Y'] for item in train])\n",
    "    X_test = np.vstack([item['X'] for item in test])\n",
    "    Y_test = np.hstack([item['Y'] for item in test])\n",
    "    \n",
    "    sgd = SGDClassifier(max_iter=10, tol=1e-3, loss='log', n_jobs=NCPU)\n",
    "    sgd.fit(X_train, Y_train)\n",
    "    \n",
    "    score_train = sgd.score(X_train, Y_train)\n",
    "    score_test = sgd.score(X_test, Y_test)\n",
    "    \n",
    "    Y_test_pred = sgd.predict_proba(X_test)\n",
    "    loss = log_loss(Y_test, Y_test_pred)\n",
    "    hloss = entropy_loss(Y_train, Y_test_pred)\n",
    "\n",
    "    print(WINDOW, score_train, score_test, loss, hloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization strength tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#params = {'l1_ratio' : np.linspace(0.0, 1.0, 11), 'alpha' : [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1] }\n",
    "params = {'l1_ratio' : np.linspace(0.0, 1.0, 11)}\n",
    "\n",
    "X_train = np.vstack([item['X'] for item in train[::10]])\n",
    "Y_train = np.hstack([item['Y'] for item in train[::10]])\n",
    "\n",
    "sgd = SGDClassifier(max_iter=100, tol=1e-3, loss='log', n_jobs=4)\n",
    "hsearch = GridSearchCV(sgd, params, cv=5, verbose=2, n_jobs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsearch.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mean, std, params in zip(means, stds, hsearch.cv_results_['params']):\n",
    "       print(\"%0.5f (+/-%0.05f) for %r\" % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

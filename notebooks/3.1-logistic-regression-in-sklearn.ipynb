{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# number of CPUs to be used\n",
    "NCPU=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json, gzip\n",
    "import numpy as np\n",
    "\n",
    "aa2idx = {'A':0, 'R':1, 'N':2, 'D':3, 'C':4, 'Q':5, 'E':6, 'G':7, 'H':8, 'I':9,\n",
    "          'L':10, 'K':11, 'M':12, 'F':13, 'P':14, 'S':15, 'T':16, 'W':17, 'Y':18, 'V':19}\n",
    "\n",
    "# read .json file\n",
    "with gzip.open('../data/phipsi.json.gz', 'rb') as f:\n",
    "    dataset = json.load(f)\n",
    "\n",
    "# reduse dataset to a list for simpler access\n",
    "dataset = dataset['phipsi10882']\n",
    "\n",
    "# convert data to numpy arrays skipping first and last residues\n",
    "for item in dataset:\n",
    "    n = len(item['sequence'])\n",
    "    item['sequence'] = np.array([aa2idx[aa] for aa in item['sequence'][1:n-1]], dtype=np.int8)\n",
    "    item['phi'] = np.array(item['phi'], dtype=np.float32)[1:n-1]\n",
    "    item['psi'] = np.array(item['psi'], dtype=np.float32)[1:n-1]\n",
    "    \n",
    "    # convert (phi,psi) to their sin() and cos()\n",
    "    # (4 numbers per angle pair)\n",
    "    item['avec'] = np.vstack([\n",
    "        np.sin(item['phi']).T,\n",
    "        np.cos(item['phi']).T,\n",
    "        np.sin(item['psi']).T,\n",
    "        np.cos(item['psi']).T ]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 9793 proteins\n",
      "Test size: 1089 proteins\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 90% train, 10% test\n",
    "train,test = train_test_split(dataset, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Train size: {} proteins\".format(len(train)))\n",
    "print(\"Test size: {} proteins\".format(len(test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means clusters from angle vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform clustering, but on the train set only. Test set will be used further to assess and tune the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.63 s, sys: 2.91 s, total: 4.54 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# combine all proteins in the training set\n",
    "# into one vector X\n",
    "X = np.vstack([item['avec'] for item in train])\n",
    "\n",
    "# split into 20 clusters\n",
    "kmeans = KMeans(n_clusters=20, max_iter=5, n_jobs=NCPU).fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now discretize dihedrals (&phi;,&psi;) in both train and test sets based on the clustering results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save cluster IDs for both train and test sets\n",
    "for subset in (train, test):\n",
    "    for item in subset:\n",
    "        item['abin'] = np.array(kmeans.predict(item['avec']), dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split sequences into chunks of equal length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_window(train, test, WINDOW):\n",
    "    for subset in (train, test):\n",
    "        for item in subset:\n",
    "            l = len(item['sequence'])\n",
    "\n",
    "            abin = item['abin']\n",
    "            seq = item['sequence']\n",
    "\n",
    "            # for every window, pick the element in the middle and\n",
    "            # save corresponding dihedral cluster ID in item['Y']\n",
    "            item['Y'] = np.hstack([item[WINDOW//2] \n",
    "                                   for shift in range(0,WINDOW,1) \n",
    "                                   for item in np.split(abin[shift:],range(0,l,WINDOW)) \n",
    "                                   if len(item) == WINDOW])\n",
    "\n",
    "            # use 1-hot encoding for every sequence chunk\n",
    "            seq_chunks = np.vstack([item for shift in range(0,WINDOW,1) \n",
    "                                    for item in np.split(seq[shift:],range(0,l,WINDOW)) \n",
    "                                    if len(item) == WINDOW])\n",
    "            item['X'] = np.array(np.eye(20)[seq_chunks], dtype=np.int8).reshape((seq_chunks.shape[0],-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test ```set_window(..)``` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence length: 80\n",
      "Number of 13-mers: 68\n",
      "Number of features: 260\n",
      "Length of Y: 68\n",
      "CPU times: user 37.6 s, sys: 316 ms, total: 38 s\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "WINDOW = 13\n",
    "\n",
    "set_window(train, test, WINDOW)\n",
    "\n",
    "item = test[42]\n",
    "print(\"Sequence length: {}\".format(len(item['sequence'])))\n",
    "print(\"Number of {}-mers: {}\".format(WINDOW, item['X'].shape[0]))\n",
    "print(\"Number of features: {}\".format(item['X'].shape[1]))\n",
    "print(\"Length of Y: {}\".format(item['Y'].shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression\n",
    "\n",
    "\n",
    "Stack X and Y vectors from all proteins together to create single train/test vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (2027748, 260)\n",
      "Y_train shape: (2027748,)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.vstack([item['X'] for item in train])\n",
    "Y_train = np.hstack([item['Y'] for item in train])\n",
    "\n",
    "X_test = np.vstack([item['X'] for item in test])\n",
    "Y_test = np.hstack([item['Y'] for item in test])\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train.shape))\n",
    "print(\"Y_train shape: {}\".format(Y_train.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying regular ```sklearn```'s ```LogisticRegression``` to the entire training set is quite time- and memory- consuming. A workaround is mini batch logistic regression accessible via ```SGDClassifier``` class with ```loss``` parameter set to ```'log'```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 22s, sys: 5.32 s, total: 8min 28s\n",
      "Wall time: 31.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "sgd = SGDClassifier(max_iter=1000, tol=1e-3, loss='log', n_jobs=NCPU)\n",
    "\n",
    "sgd.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/test scores: 0.2580946942124958 / 0.257347556402587\n"
     ]
    }
   ],
   "source": [
    "print(\"Train/test scores: {} / {}\".format(sgd.score(X_train, Y_train), sgd.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert predictions back to angles and plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.84557816  0.12254535 -0.05796351  0.13275568] 14\n"
     ]
    }
   ],
   "source": [
    "cent = kmeans.cluster_centers_\n",
    "pred = sgd.predict_proba(X_test)[12342].reshape(20,1)\n",
    "print(np.sum(pred * cent, axis=0), Y_test[12342])\n",
    "#print(sgd.predict_proba(X_test)[12342].reshape(20,1))\n",
    "#print(sgd.predict_proba(X_test)[12342].reshape(20,1) * kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(226674,) (226674, 20)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1, 13,  7, ...,  6,  9,  0], dtype=int8)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "#Yref = kmeans.predict(np.vstack([item['Y'] for item in test]))\n",
    "Y_test_pred = sgd.predict_proba(X_test)\n",
    "print(Y_test.shape, Y_test_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2648874212258696"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(Y_test,Y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
